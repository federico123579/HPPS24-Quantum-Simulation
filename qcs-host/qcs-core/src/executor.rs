//! This module collects executors, which are responsible for executing the instructions
//! generated by the scheduler.
//!
//! As for now there is only one executor, the `CpuExecutor`, which is responsible for
//! executing the instructions on the CPU.

use std::{sync::Arc, thread::JoinHandle};

use crossbeam::channel::unbounded;
use dashmap::{DashMap, DashSet};

use crate::{
    model::blocks::SpannedBlock,
    scheduler::contraction::{ContractionInstruction, ContractionOperand, ContractionPlan},
};

/// The `CpuExecutor` is responsible for executing the instructions on the CPU.
#[derive(Debug, Default)]
pub struct CpuExecutor {
    /// The memory of the executor, which is a map from the id of the block to the block itself.
    memory: Arc<DashMap<usize, SpannedBlock>>,
    threads: Vec<JoinHandle<()>>,
}

trait BlockStore {
    fn load_block(&self, operand: ContractionOperand) -> SpannedBlock;
    fn save_block(&self, id: usize, block: SpannedBlock);
    fn prepare_computation(&self, instruction: ContractionInstruction) -> Computation;
}

impl BlockStore for DashMap<usize, SpannedBlock> {
    /// Loads a block from memory or from the instruction.
    #[inline]
    fn load_block(&self, instruction: ContractionOperand) -> SpannedBlock {
        match instruction {
            ContractionOperand::Gate(gate) => gate.into(),
            ContractionOperand::Address(address) => self.remove(&address).unwrap().1,
        }
    }

    /// Saves a block in memory.
    #[inline]
    fn save_block(&self, id: usize, block: SpannedBlock) {
        self.insert(id, block);
    }

    /// Prepare the execution of a single instruction.
    fn prepare_computation(&self, instruction: ContractionInstruction) -> Computation {
        let ContractionInstruction {
            id, first, second, ..
        } = instruction;

        let first_block = self.load_block(first);
        let second_block = self.load_block(second);

        Computation {
            wid: id,
            block1: first_block,
            block2: second_block,
        }
    }
}

impl CpuExecutor {
    /// Creates a new `CpuExecutor`.
    pub fn new() -> Self {
        Self {
            memory: Arc::new(DashMap::new()),
            threads: Vec::new(),
        }
    }

    /// Spawns a new thread and keeps track of it.
    pub fn spawn<F>(&mut self, f: F)
    where
        F: FnOnce() + Send + 'static,
    {
        self.threads.push(std::thread::spawn(f))
    }

    /// Wait for all thread completion.
    pub fn join_all(&mut self) {
        for thread in self.threads.drain(..) {
            let _ = thread.join();
        }
    }

    /// Execute a contraction plan, in parallel.
    pub fn execute(mut self, mut plan: ContractionPlan) -> Vec<SpannedBlock> {
        // create a channel to communicate the need of more instructions
        let (wtx, wrx) = unbounded();
        let (itx, irx) = unbounded();
        // create a set of usize to keep track of current execution
        let executing = Arc::new(DashSet::new());

        // spawn a thread that fills the queue with the ready instructions
        self.spawn(move || {
            for instruction in plan.fetch_ready() {
                executing.insert(instruction.id);
                itx.send(instruction).unwrap();
            }
            while let Ok(id) = wrx.recv() {
                plan.set_done(std::iter::once(id));
                for instruction in plan
                    .fetch_ready()
                    .into_iter()
                    .filter(|i| executing.insert(i.id))
                {
                    itx.send(instruction).unwrap();
                }
                if plan.is_empty() {
                    break;
                }
            }
        });

        // spawn many thread as cpu cores to execute the instructions
        for _ in 0..num_cpus::get() {
            let wtx_clone = wtx.clone();
            let irx_clone = irx.clone();
            let block_map = Arc::clone(&self.memory);
            self.spawn(move || {
                while let Ok(instruction) = irx_clone.recv() {
                    let computation = block_map.prepare_computation(instruction);
                    let (id, result) = computation.compute();
                    block_map.save_block(id, result);
                    if wtx_clone.send(id).is_err() {
                        break;
                    }
                }
            });
        }

        // wait for all threads to finish
        self.join_all();

        Arc::try_unwrap(self.memory)
            .unwrap()
            .into_iter()
            .map(|(_, block)| block)
            .collect()
    }
}

struct Computation {
    wid: usize,
    block1: SpannedBlock,
    block2: SpannedBlock,
}

impl Computation {
    fn compute(self) -> (usize, SpannedBlock) {
        let new_span = self.block1.merged_span(&self.block2);
        let first_block = self.block1.adapt_to_span(new_span.clone());
        let second_block: SpannedBlock = self.block2.adapt_to_span(new_span);
        (self.wid, first_block * second_block)
    }
}
